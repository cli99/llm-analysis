{
    "batch_size_per_gpu": 50,
    "max_batch_size_per_gpu": 62,
    "gradient_accumulation_steps": 2500,
    "global_batch_size": 4000000,
    "dp_size": 32,
    "tp_size": 8,
    "pp_size": 8,
    "sp_size": 8,
    "ds_zero": "NONE",
    "total_num_gpus": 2048,
    "seq_len": 2048,
    "total_num_tokens": 1000000000000.0,
    "num_params_total": 6573522944,
    "activation_recomputation": "SELECTIVE",
    "achived_flops": 494.5,
    "flops_efficiency": 0.5,
    "hbm_memory_efficiency": 0.9,
    "num_flops_total_per_micro_batch": 4478577147904000,
    "weight_memory_per_gpu": 234094592.0,
    "gradient_memory_per_gpu": 201326592.0,
    "optimizer_state_memory_per_gpu": 1610612736.0,
    "activation_memory_bs1": 1241513984.0,
    "activation_memory_per_gpu": 62075699200.0,
    "latency_per_micro_batch": 0.14151216973913044,
    "latency_fwd": 0.08195352907946148,
    "latency_fwd_attn": 0.01737094962992922,
    "latency_fwd_mlp": 0.027793519407886755,
    "latency_fwd_layernorm": 0.0005564582421227198,
    "latency_fwd_tp_comm": 0.026097891555555554,
    "latency_fwd_input_embedding": 0.0033491830447761194,
    "latency_fwd_output_embedding_loss": 0.006785527199191102,
    "latency_per_iter": 353.7804243478261,
    "total_training_latency": 43161.211770434784,
    "gpu_hours": 24553.933807180678
}
